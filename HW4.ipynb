{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW: X-ray images classification\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, open Mobaxterm and connect to triton with the user and password you were give with. Activate the environment `2ndPaper` and then type the command `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will be dealing with classification of 32X32 X-ray images of the chest. The image can be classified into one of four options: lungs (l), clavicles (c), and heart (h) and background (b). Even though those labels are dependent, we will treat this task as multiclass and not as multilabel. The dataset for this assignment is located on a shared folder on triton (`/MLdata/MLcourse/X_ray/'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, MaxPool2D, Conv2D, Dropout\n",
    "from tensorflow.keras.layers import Flatten, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto(gpu_options =\n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        src = imread(os.path.join(datapath, fn),1)\n",
    "        img = resize(src,(32,32),order = 3)\n",
    "        \n",
    "        images[ii,:,:,0] = img\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    BaseImages = images\n",
    "    BaseY = Y\n",
    "    return BaseImages, BaseY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_and_val(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        images[ii,:,:,0] = imread(os.path.join(datapath, fn),1)\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    return images, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data for training and validation:\n",
    "#src_data = '/MLdata/MLcourse/X_ray/'\n",
    "src_data = 'C:\\\\Users\\\\ofeka\\\\Desktop\\\\Projects\\\\Machine Learning\\\\Homework\\\\HW4\\\\X_ray\\\\'\n",
    "train_path = src_data + 'train'\n",
    "val_path = src_data + 'validation'\n",
    "test_path = src_data + 'test'\n",
    "BaseX_train , BaseY_train = preprocess_train_and_val(train_path)\n",
    "BaseX_val , BaseY_val = preprocess_train_and_val(val_path)\n",
    "X_test, Y_test = preprocess(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6474, 32, 32, 1)\n",
      "(6629376,)\n",
      "(6474, 4)\n",
      "(1728, 32, 32, 1)\n",
      "(1728, 4)\n",
      "(175, 32, 32, 1)\n",
      "(175, 4)\n"
     ]
    }
   ],
   "source": [
    "# BLOCK FOR TESTING STUFF:\n",
    "\n",
    "print(BaseX_train.shape)\n",
    "print(BaseX_train.flatten().shape)\n",
    "print(BaseY_train.shape)\n",
    "print(BaseX_val.shape)\n",
    "print(BaseY_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Fully connected layers \n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *NN with fully connected layers. \n",
    "\n",
    "Elaborate a NN with 2 hidden fully connected layers with 300, 150 neurons and 4 neurons for classification. Use ReLU activation functions for the hidden layers and He_normal for initialization. Don't forget to flatten your image before feedforward to the first dense layer. Name the model `model_relu`.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "#Data Flattening:\n",
    "BaseX_train_f = BaseX_train.flatten()\n",
    "BaseY_train_f = BaseY_train.flatten()\n",
    "BaseX_val_f = BaseX_val.flatten()\n",
    "BaseY_val_f = BaseY_val.flatten()\n",
    "X_test_f = X_test.flatten()\n",
    "Y_test_f = Y_test.flatten()\n",
    "\n",
    "\n",
    "n_filters_start = 300\n",
    "n_filters_finish = 4\n",
    "len_sub_window = 10\n",
    "dropout = 0.2\n",
    "he_normal_initializer = tf.keras.initializers.he_normal()\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape = (32,32,1))) #input_shape defines the shape of the input: 32X32 is the image size, while 3 is the number of dimensions (RGB hence 3)\n",
    "model_relu.add(Dense(n_filters_start, activation='relu',kernel_initializer = he_normal_initializer)) \n",
    "model_relu.add(Dropout(dropout))\n",
    "model_relu.add(Dense(n_filters_start/2,activation='relu'))\n",
    "model_relu.add(Dropout(dropout))\n",
    "model_relu.add(Dense(n_filters_finish,activation='softmax'))\n",
    "\n",
    "\n",
    "# another try model\n",
    "# model_relu_d = Sequential(name=\"model_relu\")\n",
    "# model_relu_d.add(Dense(300, input_shape=(32 ** 2,), kernel_initializer=\"he_normal\"))\n",
    "# model_relu_d.add(Activation('relu', name='ReLU_1'))\n",
    "# model_relu_d.add(Dropout(0.2))\n",
    "\n",
    "# model_relu_d.add(Dense(150, kernel_initializer=\"he_normal\"))\n",
    "# model_relu_d.add(Activation('relu', name='ReLU_2'))\n",
    "# model_relu_d.add(Dropout(0.2))\n",
    "\n",
    "# model_relu_d.add(Dense(4))\n",
    "# model_relu_d.add(Activation('softmax'))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with the optimizer above, accuracy metric and adequate loss for multiclass task. Train your model on the training set and evaluate the model on the testing set. Print the accuracy and loss over the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 175 samples\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 1s 166us/sample - loss: 1.3627 - accuracy: 0.3163 - val_loss: 1.2818 - val_accuracy: 0.4686\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 1s 84us/sample - loss: 1.2486 - accuracy: 0.4526 - val_loss: 1.2136 - val_accuracy: 0.5486\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 1s 85us/sample - loss: 1.1663 - accuracy: 0.5236 - val_loss: 1.1428 - val_accuracy: 0.6000\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 1.1051 - accuracy: 0.5635 - val_loss: 1.1041 - val_accuracy: 0.6114\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 1s 82us/sample - loss: 1.0597 - accuracy: 0.5941 - val_loss: 1.0637 - val_accuracy: 0.6343\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 1.0203 - accuracy: 0.6180 - val_loss: 1.0352 - val_accuracy: 0.6114\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 1s 83us/sample - loss: 0.9918 - accuracy: 0.6305 - val_loss: 1.0027 - val_accuracy: 0.6286\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 1s 82us/sample - loss: 0.9575 - accuracy: 0.6548 - val_loss: 0.9760 - val_accuracy: 0.6343\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 1s 82us/sample - loss: 0.9342 - accuracy: 0.6636 - val_loss: 0.9484 - val_accuracy: 0.6571\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 1s 81us/sample - loss: 0.9064 - accuracy: 0.6813 - val_loss: 0.9336 - val_accuracy: 0.6571\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 1s 82us/sample - loss: 0.8880 - accuracy: 0.6894 - val_loss: 0.9191 - val_accuracy: 0.6457\n",
      "Epoch 12/25\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.8613 - accuracy: 0.7033 - val_loss: 0.9016 - val_accuracy: 0.6571\n",
      "Epoch 13/25\n",
      "6474/6474 [==============================] - 1s 87us/sample - loss: 0.8436 - accuracy: 0.7108 - val_loss: 0.8895 - val_accuracy: 0.6286\n",
      "Epoch 14/25\n",
      "6474/6474 [==============================] - 1s 82us/sample - loss: 0.8295 - accuracy: 0.7181 - val_loss: 0.8763 - val_accuracy: 0.6400\n",
      "Epoch 15/25\n",
      "6474/6474 [==============================] - 1s 110us/sample - loss: 0.8061 - accuracy: 0.7237 - val_loss: 0.8655 - val_accuracy: 0.6514\n",
      "Epoch 16/25\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.7977 - accuracy: 0.7368 - val_loss: 0.8557 - val_accuracy: 0.6629\n",
      "Epoch 17/25\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.7803 - accuracy: 0.7345 - val_loss: 0.8484 - val_accuracy: 0.6571\n",
      "Epoch 18/25\n",
      "6474/6474 [==============================] - 1s 91us/sample - loss: 0.7584 - accuracy: 0.7533 - val_loss: 0.8401 - val_accuracy: 0.6571\n",
      "Epoch 19/25\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.7517 - accuracy: 0.7487 - val_loss: 0.8347 - val_accuracy: 0.6571\n",
      "Epoch 20/25\n",
      "6474/6474 [==============================] - 1s 92us/sample - loss: 0.7312 - accuracy: 0.7655 - val_loss: 0.8258 - val_accuracy: 0.6686\n",
      "Epoch 21/25\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.7212 - accuracy: 0.7634 - val_loss: 0.8210 - val_accuracy: 0.6571\n",
      "Epoch 22/25\n",
      "6474/6474 [==============================] - 1s 96us/sample - loss: 0.7102 - accuracy: 0.7709 - val_loss: 0.8165 - val_accuracy: 0.6743\n",
      "Epoch 23/25\n",
      "6474/6474 [==============================] - 1s 94us/sample - loss: 0.6982 - accuracy: 0.7702 - val_loss: 0.8151 - val_accuracy: 0.6571\n",
      "Epoch 24/25\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 0.6832 - accuracy: 0.7754 - val_loss: 0.8096 - val_accuracy: 0.6514\n",
      "Epoch 25/25\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.6726 - accuracy: 0.7776 - val_loss: 0.8058 - val_accuracy: 0.6743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b896caa808>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer = AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "model_relu.fit(BaseX_train, BaseY_train, epochs = epochs, batch_size = batch_size, validation_data=(X_test,Y_test))\n",
    "\n",
    "# This crossentropy loss function is used when there are two or more label classes. We expect labels to be provided in a one_hot representation. \n",
    "# If labels were  provided as integers, then SparseCategoricalCrossentropy loss should be used. There should be # classes floating point values per feature.\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 114us/sample - loss: 0.8058 - accuracy: 0.6743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8058029648235866, 0.6742857]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_relu.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Activation functions.* \n",
    "\n",
    "Change the activation functions to LeakyRelu or tanh or sigmoid. Name the new model `new_a_model`. Explain how it can affect the model.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "#import keras.layers.advanced_activations as advanced_activations\n",
    "leaky_relu = LeakyReLU(alpha=0.1)\n",
    "\n",
    "\n",
    "new_a_model = Sequential()\n",
    "new_a_model.add(Flatten(input_shape = (32,32,1))) #input_shape defines the shape of the input: 32X32 is the image size, while 3 is the number of dimensions (RGB hence 3)\n",
    "new_a_model.add(leaky_relu)\n",
    "new_a_model.add(Dense(n_filters_start,kernel_initializer = he_normal_initializer)) \n",
    "new_a_model.add(leaky_relu)\n",
    "new_a_model.add(Dropout(dropout))\n",
    "new_a_model.add(Dense(n_filters_start/2))\n",
    "new_a_model.add(leaky_relu)\n",
    "new_a_model.add(Dropout(dropout))\n",
    "new_a_model.add(Dense(n_filters_finish))\n",
    "new_a_model.add(Dense(n_filters_finish,activation='softmax'))\n",
    "\n",
    "new_a_model_25 = new_a_model\n",
    "new_a_model_40 = new_a_model\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 4)                 604       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 353,274\n",
      "Trainable params: 353,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_a_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 3:***</span> *Number of epochs.* \n",
    "\n",
    "Train the new model using 25 and 40 epochs. What difference does it makes in term of performance? Remember to save the compiled model for having initialized weights for every run as we did in tutorial 12. Evaluate each trained model on the test set*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 175 samples\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 1s 208us/sample - loss: 0.4434 - accuracy: 0.8550 - val_loss: 0.8951 - val_accuracy: 0.6514\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 1s 87us/sample - loss: 0.4432 - accuracy: 0.8551 - val_loss: 0.8767 - val_accuracy: 0.6571\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 1s 87us/sample - loss: 0.4420 - accuracy: 0.8562 - val_loss: 0.8842 - val_accuracy: 0.6514\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 1s 86us/sample - loss: 0.4377 - accuracy: 0.8585 - val_loss: 0.8858 - val_accuracy: 0.6514\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 1s 94us/sample - loss: 0.4343 - accuracy: 0.8610 - val_loss: 0.8938 - val_accuracy: 0.6514\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 1s 93us/sample - loss: 0.4311 - accuracy: 0.8636 - val_loss: 0.8899 - val_accuracy: 0.6571\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 1s 91us/sample - loss: 0.4313 - accuracy: 0.8577 - val_loss: 0.8869 - val_accuracy: 0.6514\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 1s 98us/sample - loss: 0.4342 - accuracy: 0.8582 - val_loss: 0.8907 - val_accuracy: 0.6514\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 1s 91us/sample - loss: 0.4233 - accuracy: 0.8610 - val_loss: 0.8897 - val_accuracy: 0.6514\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 1s 91us/sample - loss: 0.4262 - accuracy: 0.8582 - val_loss: 0.8946 - val_accuracy: 0.6457\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.4248 - accuracy: 0.8636 - val_loss: 0.8973 - val_accuracy: 0.6457\n",
      "Epoch 12/25\n",
      "6474/6474 [==============================] - 1s 91us/sample - loss: 0.4194 - accuracy: 0.8647 - val_loss: 0.8979 - val_accuracy: 0.6457\n",
      "Epoch 13/25\n",
      "6474/6474 [==============================] - 1s 94us/sample - loss: 0.4148 - accuracy: 0.8622 - val_loss: 0.9052 - val_accuracy: 0.6457\n",
      "Epoch 14/25\n",
      "6474/6474 [==============================] - 1s 96us/sample - loss: 0.4173 - accuracy: 0.8679 - val_loss: 0.9021 - val_accuracy: 0.6457\n",
      "Epoch 15/25\n",
      "6474/6474 [==============================] - 1s 116us/sample - loss: 0.4147 - accuracy: 0.8630 - val_loss: 0.9037 - val_accuracy: 0.6514\n",
      "Epoch 16/25\n",
      "6474/6474 [==============================] - 1s 105us/sample - loss: 0.4108 - accuracy: 0.8630 - val_loss: 0.9131 - val_accuracy: 0.6514\n",
      "Epoch 17/25\n",
      "6474/6474 [==============================] - 1s 87us/sample - loss: 0.4112 - accuracy: 0.8682 - val_loss: 0.8966 - val_accuracy: 0.6457\n",
      "Epoch 18/25\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.4070 - accuracy: 0.8627 - val_loss: 0.9022 - val_accuracy: 0.6457\n",
      "Epoch 19/25\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.4064 - accuracy: 0.8673 - val_loss: 0.9157 - val_accuracy: 0.6400\n",
      "Epoch 20/25\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 0.3999 - accuracy: 0.8667 - val_loss: 0.9166 - val_accuracy: 0.6400\n",
      "Epoch 21/25\n",
      "6474/6474 [==============================] - 1s 98us/sample - loss: 0.4021 - accuracy: 0.8693 - val_loss: 0.9209 - val_accuracy: 0.6457\n",
      "Epoch 22/25\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.4020 - accuracy: 0.8693 - val_loss: 0.9194 - val_accuracy: 0.6457\n",
      "Epoch 23/25\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.3963 - accuracy: 0.8699 - val_loss: 0.9305 - val_accuracy: 0.6457\n",
      "Epoch 24/25\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 0.3945 - accuracy: 0.8712 - val_loss: 0.9174 - val_accuracy: 0.6400\n",
      "Epoch 25/25\n",
      "6474/6474 [==============================] - 1s 89us/sample - loss: 0.3905 - accuracy: 0.8716 - val_loss: 0.9243 - val_accuracy: 0.6457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b89ddcc208>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model_25.compile(optimizer = AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "new_a_model_25.fit(BaseX_train, BaseY_train, epochs = epochs, batch_size = batch_size, validation_data=(X_test,Y_test))\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 175 samples\n",
      "Epoch 1/40\n",
      "6474/6474 [==============================] - 1s 179us/sample - loss: 0.5675 - accuracy: 0.8156 - val_loss: 0.8161 - val_accuracy: 0.6743\n",
      "Epoch 2/40\n",
      "6474/6474 [==============================] - 1s 86us/sample - loss: 0.5596 - accuracy: 0.8182 - val_loss: 0.8211 - val_accuracy: 0.6686\n",
      "Epoch 3/40\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.5545 - accuracy: 0.8173 - val_loss: 0.8168 - val_accuracy: 0.6857\n",
      "Epoch 4/40\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 0.5521 - accuracy: 0.8182 - val_loss: 0.8135 - val_accuracy: 0.6743\n",
      "Epoch 5/40\n",
      "6474/6474 [==============================] - 1s 96us/sample - loss: 0.5504 - accuracy: 0.8233 - val_loss: 0.8220 - val_accuracy: 0.6743\n",
      "Epoch 6/40\n",
      "6474/6474 [==============================] - 1s 96us/sample - loss: 0.5478 - accuracy: 0.8225 - val_loss: 0.8344 - val_accuracy: 0.6629\n",
      "Epoch 7/40\n",
      "6474/6474 [==============================] - 1s 92us/sample - loss: 0.5432 - accuracy: 0.8244 - val_loss: 0.8225 - val_accuracy: 0.6743\n",
      "Epoch 8/40\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.5288 - accuracy: 0.8265 - val_loss: 0.8307 - val_accuracy: 0.6629\n",
      "Epoch 9/40\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.5362 - accuracy: 0.8241 - val_loss: 0.8310 - val_accuracy: 0.6686\n",
      "Epoch 10/40\n",
      "6474/6474 [==============================] - 1s 94us/sample - loss: 0.5297 - accuracy: 0.8255 - val_loss: 0.8320 - val_accuracy: 0.6629\n",
      "Epoch 11/40\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.5252 - accuracy: 0.8326 - val_loss: 0.8351 - val_accuracy: 0.6629\n",
      "Epoch 12/40\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.5189 - accuracy: 0.8347 - val_loss: 0.8410 - val_accuracy: 0.6629\n",
      "Epoch 13/40\n",
      "6474/6474 [==============================] - 1s 97us/sample - loss: 0.5178 - accuracy: 0.8290 - val_loss: 0.8369 - val_accuracy: 0.6686\n",
      "Epoch 14/40\n",
      "6474/6474 [==============================] - 1s 92us/sample - loss: 0.5168 - accuracy: 0.8323 - val_loss: 0.8299 - val_accuracy: 0.6743\n",
      "Epoch 15/40\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.5187 - accuracy: 0.8275 - val_loss: 0.8431 - val_accuracy: 0.6571\n",
      "Epoch 16/40\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.5064 - accuracy: 0.8367 - val_loss: 0.8407 - val_accuracy: 0.6571\n",
      "Epoch 17/40\n",
      "6474/6474 [==============================] - 1s 89us/sample - loss: 0.5057 - accuracy: 0.8353 - val_loss: 0.8379 - val_accuracy: 0.6686\n",
      "Epoch 18/40\n",
      "6474/6474 [==============================] - 1s 107us/sample - loss: 0.5050 - accuracy: 0.8353 - val_loss: 0.8407 - val_accuracy: 0.6686\n",
      "Epoch 19/40\n",
      "6474/6474 [==============================] - 1s 91us/sample - loss: 0.4993 - accuracy: 0.8387 - val_loss: 0.8415 - val_accuracy: 0.6686\n",
      "Epoch 20/40\n",
      "6474/6474 [==============================] - 1s 87us/sample - loss: 0.4956 - accuracy: 0.8394 - val_loss: 0.8399 - val_accuracy: 0.6686\n",
      "Epoch 21/40\n",
      "6474/6474 [==============================] - 1s 89us/sample - loss: 0.4852 - accuracy: 0.8424 - val_loss: 0.8491 - val_accuracy: 0.6629\n",
      "Epoch 22/40\n",
      "6474/6474 [==============================] - 1s 96us/sample - loss: 0.4878 - accuracy: 0.8384 - val_loss: 0.8345 - val_accuracy: 0.6629\n",
      "Epoch 23/40\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 0.4840 - accuracy: 0.8414 - val_loss: 0.8606 - val_accuracy: 0.6514\n",
      "Epoch 24/40\n",
      "6474/6474 [==============================] - 1s 93us/sample - loss: 0.4890 - accuracy: 0.8360 - val_loss: 0.8484 - val_accuracy: 0.6629\n",
      "Epoch 25/40\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.4891 - accuracy: 0.8400 - val_loss: 0.8547 - val_accuracy: 0.6629\n",
      "Epoch 26/40\n",
      "6474/6474 [==============================] - 1s 89us/sample - loss: 0.4756 - accuracy: 0.8446 - val_loss: 0.8607 - val_accuracy: 0.6571\n",
      "Epoch 27/40\n",
      "6474/6474 [==============================] - 1s 91us/sample - loss: 0.4752 - accuracy: 0.8451 - val_loss: 0.8582 - val_accuracy: 0.6514\n",
      "Epoch 28/40\n",
      "6474/6474 [==============================] - 1s 85us/sample - loss: 0.4771 - accuracy: 0.8432 - val_loss: 0.8609 - val_accuracy: 0.6686\n",
      "Epoch 29/40\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.4717 - accuracy: 0.8452 - val_loss: 0.8643 - val_accuracy: 0.6514\n",
      "Epoch 30/40\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.4676 - accuracy: 0.8503 - val_loss: 0.8729 - val_accuracy: 0.6514\n",
      "Epoch 31/40\n",
      "6474/6474 [==============================] - 1s 89us/sample - loss: 0.4647 - accuracy: 0.8465 - val_loss: 0.8698 - val_accuracy: 0.6457\n",
      "Epoch 32/40\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 0.4625 - accuracy: 0.8458 - val_loss: 0.8617 - val_accuracy: 0.6514\n",
      "Epoch 33/40\n",
      "6474/6474 [==============================] - 1s 94us/sample - loss: 0.4550 - accuracy: 0.8491 - val_loss: 0.8740 - val_accuracy: 0.6743\n",
      "Epoch 34/40\n",
      "6474/6474 [==============================] - 1s 92us/sample - loss: 0.4584 - accuracy: 0.8526 - val_loss: 0.8668 - val_accuracy: 0.6686\n",
      "Epoch 35/40\n",
      "6474/6474 [==============================] - 1s 86us/sample - loss: 0.4632 - accuracy: 0.8497 - val_loss: 0.8735 - val_accuracy: 0.6514\n",
      "Epoch 36/40\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 0.4529 - accuracy: 0.8559 - val_loss: 0.8718 - val_accuracy: 0.6571\n",
      "Epoch 37/40\n",
      "6474/6474 [==============================] - 1s 89us/sample - loss: 0.4494 - accuracy: 0.8519 - val_loss: 0.8754 - val_accuracy: 0.6514\n",
      "Epoch 38/40\n",
      "6474/6474 [==============================] - 1s 89us/sample - loss: 0.4510 - accuracy: 0.8536 - val_loss: 0.8817 - val_accuracy: 0.6514\n",
      "Epoch 39/40\n",
      "6474/6474 [==============================] - 1s 88us/sample - loss: 0.4483 - accuracy: 0.8543 - val_loss: 0.8795 - val_accuracy: 0.6571\n",
      "Epoch 40/40\n",
      "6474/6474 [==============================] - 1s 90us/sample - loss: 0.4485 - accuracy: 0.8543 - val_loss: 0.8830 - val_accuracy: 0.6629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b89dbc17c8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "new_a_model_40.compile(optimizer = AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "new_a_model_40.fit(BaseX_train, BaseY_train, epochs = epochs, batch_size = batch_size, validation_data=(X_test,Y_test))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 131us/sample - loss: 0.9243 - accuracy: 0.6457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9242858869688851, 0.6457143]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a_model_25.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 120us/sample - loss: 0.9243 - accuracy: 0.6457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9242858869688851, 0.6457143]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a_model_40.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at results/model_relu.h5 \n",
      "Saved trained model at results/new_a_model_25.h5 \n",
      "Saved trained model at results/new_a_model_40.h5 \n"
     ]
    }
   ],
   "source": [
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "\n",
    "#model_relu:\n",
    "model_name = \"model_relu.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "#new_a_model - 25 epochs:\n",
    "model_name = \"new_a_model_25.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model_25.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "\n",
    "#new_a_model - 40 epochs:\n",
    "model_name = \"new_a_model_40.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model_40.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Mini-batches.* \n",
    "\n",
    "Build the `model_relu` again and run it with a batch size of 32 instead of 64. What are the advantages of the mini-batch vs. SGD?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "# 64 batch-size:\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape = (32,32,1))) #input_shape defines the shape of the input: 32X32 is the image size, while 3 is the number of dimensions (RGB hence 3)\n",
    "model_relu.add(Dense(n_filters_start, activation='relu',kernel_initializer = he_normal_initializer)) \n",
    "model_relu.add(Dropout(dropout))\n",
    "model_relu.add(Dense(n_filters_start/2,activation='relu'))\n",
    "model_relu.add(Dropout(dropout))\n",
    "model_relu.add(Dense(n_filters_finish,activation='softmax'))\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 175 samples\n",
      "Epoch 1/50\n",
      "6474/6474 [==============================] - 2s 239us/sample - loss: 1.3167 - accuracy: 0.3665 - val_loss: 1.2095 - val_accuracy: 0.5829\n",
      "Epoch 2/50\n",
      "6474/6474 [==============================] - 1s 151us/sample - loss: 1.1841 - accuracy: 0.5017 - val_loss: 1.1213 - val_accuracy: 0.6229\n",
      "Epoch 3/50\n",
      "6474/6474 [==============================] - 1s 153us/sample - loss: 1.1023 - accuracy: 0.5650 - val_loss: 1.0568 - val_accuracy: 0.6343\n",
      "Epoch 4/50\n",
      "6474/6474 [==============================] - 1s 146us/sample - loss: 1.0406 - accuracy: 0.6129 - val_loss: 1.0129 - val_accuracy: 0.6457\n",
      "Epoch 5/50\n",
      "6474/6474 [==============================] - 1s 147us/sample - loss: 0.9877 - accuracy: 0.6358 - val_loss: 0.9767 - val_accuracy: 0.6343\n",
      "Epoch 6/50\n",
      "6474/6474 [==============================] - 1s 146us/sample - loss: 0.9464 - accuracy: 0.6594 - val_loss: 0.9445 - val_accuracy: 0.6457\n",
      "Epoch 7/50\n",
      "6474/6474 [==============================] - 1s 157us/sample - loss: 0.9085 - accuracy: 0.6801 - val_loss: 0.9268 - val_accuracy: 0.6400\n",
      "Epoch 8/50\n",
      "6474/6474 [==============================] - 1s 147us/sample - loss: 0.8725 - accuracy: 0.7027 - val_loss: 0.9090 - val_accuracy: 0.6343\n",
      "Epoch 9/50\n",
      "6474/6474 [==============================] - 1s 164us/sample - loss: 0.8539 - accuracy: 0.7101 - val_loss: 0.8943 - val_accuracy: 0.6514\n",
      "Epoch 10/50\n",
      "6474/6474 [==============================] - 1s 161us/sample - loss: 0.8232 - accuracy: 0.7243 - val_loss: 0.8814 - val_accuracy: 0.6457\n",
      "Epoch 11/50\n",
      "6474/6474 [==============================] - 1s 134us/sample - loss: 0.8022 - accuracy: 0.7257 - val_loss: 0.8683 - val_accuracy: 0.6400\n",
      "Epoch 12/50\n",
      "6474/6474 [==============================] - 1s 176us/sample - loss: 0.7777 - accuracy: 0.7362 - val_loss: 0.8619 - val_accuracy: 0.6514\n",
      "Epoch 13/50\n",
      "6474/6474 [==============================] - 1s 160us/sample - loss: 0.7565 - accuracy: 0.7468 - val_loss: 0.8479 - val_accuracy: 0.6686\n",
      "Epoch 14/50\n",
      "6474/6474 [==============================] - 1s 152us/sample - loss: 0.7404 - accuracy: 0.7549 - val_loss: 0.8452 - val_accuracy: 0.6743\n",
      "Epoch 15/50\n",
      "6474/6474 [==============================] - 1s 136us/sample - loss: 0.7239 - accuracy: 0.7583 - val_loss: 0.8405 - val_accuracy: 0.6629\n",
      "Epoch 16/50\n",
      "6474/6474 [==============================] - 1s 158us/sample - loss: 0.7004 - accuracy: 0.7688 - val_loss: 0.8337 - val_accuracy: 0.6629\n",
      "Epoch 17/50\n",
      "6474/6474 [==============================] - 1s 163us/sample - loss: 0.6932 - accuracy: 0.7706 - val_loss: 0.8337 - val_accuracy: 0.6629\n",
      "Epoch 18/50\n",
      "6474/6474 [==============================] - 1s 149us/sample - loss: 0.6714 - accuracy: 0.7814 - val_loss: 0.8311 - val_accuracy: 0.6686\n",
      "Epoch 19/50\n",
      "6474/6474 [==============================] - 1s 181us/sample - loss: 0.6588 - accuracy: 0.7830 - val_loss: 0.8278 - val_accuracy: 0.6571\n",
      "Epoch 20/50\n",
      "6474/6474 [==============================] - 1s 197us/sample - loss: 0.6460 - accuracy: 0.7904 - val_loss: 0.8240 - val_accuracy: 0.6800\n",
      "Epoch 21/50\n",
      "6474/6474 [==============================] - 1s 152us/sample - loss: 0.6322 - accuracy: 0.7899 - val_loss: 0.8245 - val_accuracy: 0.6514\n",
      "Epoch 22/50\n",
      "6474/6474 [==============================] - 1s 139us/sample - loss: 0.6196 - accuracy: 0.7964 - val_loss: 0.8232 - val_accuracy: 0.6571\n",
      "Epoch 23/50\n",
      "6474/6474 [==============================] - 1s 122us/sample - loss: 0.6075 - accuracy: 0.7992 - val_loss: 0.8237 - val_accuracy: 0.6743\n",
      "Epoch 24/50\n",
      "6474/6474 [==============================] - 1s 121us/sample - loss: 0.5971 - accuracy: 0.8071 - val_loss: 0.8254 - val_accuracy: 0.6629\n",
      "Epoch 25/50\n",
      "6474/6474 [==============================] - 1s 121us/sample - loss: 0.5837 - accuracy: 0.8072 - val_loss: 0.8163 - val_accuracy: 0.6857\n",
      "Epoch 26/50\n",
      "6474/6474 [==============================] - 1s 128us/sample - loss: 0.5717 - accuracy: 0.8151 - val_loss: 0.8213 - val_accuracy: 0.6857\n",
      "Epoch 27/50\n",
      "6474/6474 [==============================] - 1s 137us/sample - loss: 0.5633 - accuracy: 0.8200 - val_loss: 0.8160 - val_accuracy: 0.6971\n",
      "Epoch 28/50\n",
      "6474/6474 [==============================] - 1s 127us/sample - loss: 0.5522 - accuracy: 0.8213 - val_loss: 0.8226 - val_accuracy: 0.6914\n",
      "Epoch 29/50\n",
      "6474/6474 [==============================] - 1s 120us/sample - loss: 0.5437 - accuracy: 0.8202 - val_loss: 0.8265 - val_accuracy: 0.6800\n",
      "Epoch 30/50\n",
      "6474/6474 [==============================] - 1s 121us/sample - loss: 0.5339 - accuracy: 0.8225 - val_loss: 0.8265 - val_accuracy: 0.6857\n",
      "Epoch 31/50\n",
      "6474/6474 [==============================] - 1s 132us/sample - loss: 0.5306 - accuracy: 0.8270 - val_loss: 0.8244 - val_accuracy: 0.6971\n",
      "Epoch 32/50\n",
      "6474/6474 [==============================] - 1s 139us/sample - loss: 0.5173 - accuracy: 0.8278 - val_loss: 0.8244 - val_accuracy: 0.6743\n",
      "Epoch 33/50\n",
      "6474/6474 [==============================] - 1s 147us/sample - loss: 0.5089 - accuracy: 0.8352 - val_loss: 0.8215 - val_accuracy: 0.6971\n",
      "Epoch 34/50\n",
      "6474/6474 [==============================] - 1s 126us/sample - loss: 0.5024 - accuracy: 0.8349 - val_loss: 0.8266 - val_accuracy: 0.6914\n",
      "Epoch 35/50\n",
      "6474/6474 [==============================] - 1s 128us/sample - loss: 0.4902 - accuracy: 0.8414 - val_loss: 0.8254 - val_accuracy: 0.6800\n",
      "Epoch 36/50\n",
      "6474/6474 [==============================] - 1s 128us/sample - loss: 0.4832 - accuracy: 0.8411 - val_loss: 0.8244 - val_accuracy: 0.6914\n",
      "Epoch 37/50\n",
      "6474/6474 [==============================] - 1s 125us/sample - loss: 0.4785 - accuracy: 0.8438 - val_loss: 0.8292 - val_accuracy: 0.6800\n",
      "Epoch 38/50\n",
      "6474/6474 [==============================] - 1s 116us/sample - loss: 0.4718 - accuracy: 0.8503 - val_loss: 0.8290 - val_accuracy: 0.6743\n",
      "Epoch 39/50\n",
      "6474/6474 [==============================] - 1s 129us/sample - loss: 0.4672 - accuracy: 0.8458 - val_loss: 0.8313 - val_accuracy: 0.6743\n",
      "Epoch 40/50\n",
      "6474/6474 [==============================] - 1s 130us/sample - loss: 0.4613 - accuracy: 0.8503 - val_loss: 0.8296 - val_accuracy: 0.6800\n",
      "Epoch 41/50\n",
      "6474/6474 [==============================] - 1s 125us/sample - loss: 0.4585 - accuracy: 0.8519 - val_loss: 0.8384 - val_accuracy: 0.6857\n",
      "Epoch 42/50\n",
      "6474/6474 [==============================] - 1s 128us/sample - loss: 0.4503 - accuracy: 0.8553 - val_loss: 0.8374 - val_accuracy: 0.6686\n",
      "Epoch 43/50\n",
      "6474/6474 [==============================] - 1s 134us/sample - loss: 0.4479 - accuracy: 0.8560 - val_loss: 0.8375 - val_accuracy: 0.6800\n",
      "Epoch 44/50\n",
      "6474/6474 [==============================] - 1s 128us/sample - loss: 0.4414 - accuracy: 0.8587 - val_loss: 0.8403 - val_accuracy: 0.6743\n",
      "Epoch 45/50\n",
      "6474/6474 [==============================] - 1s 128us/sample - loss: 0.4368 - accuracy: 0.8546 - val_loss: 0.8436 - val_accuracy: 0.6686\n",
      "Epoch 46/50\n",
      "6474/6474 [==============================] - 1s 128us/sample - loss: 0.4320 - accuracy: 0.8573 - val_loss: 0.8474 - val_accuracy: 0.6800\n",
      "Epoch 47/50\n",
      "6474/6474 [==============================] - 1s 129us/sample - loss: 0.4309 - accuracy: 0.8608 - val_loss: 0.8495 - val_accuracy: 0.6686\n",
      "Epoch 48/50\n",
      "6474/6474 [==============================] - 1s 127us/sample - loss: 0.4191 - accuracy: 0.8618 - val_loss: 0.8580 - val_accuracy: 0.6743\n",
      "Epoch 49/50\n",
      "6474/6474 [==============================] - 1s 129us/sample - loss: 0.4230 - accuracy: 0.8607 - val_loss: 0.8528 - val_accuracy: 0.6686\n",
      "Epoch 50/50\n",
      "6474/6474 [==============================] - 1s 123us/sample - loss: 0.4131 - accuracy: 0.8672 - val_loss: 0.8532 - val_accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b89e3a30c8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer = AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "model_relu.fit(BaseX_train, BaseY_train, epochs = epochs, batch_size = batch_size, validation_data=(X_test,Y_test))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 130us/sample - loss: 0.8532 - accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8531896362985883, 0.68]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_relu.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Batch normalization.* \n",
    "\n",
    "Build the `new_a_model` again and add batch normalization layers. How does it impact your results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model_batch = Sequential()\n",
    "new_a_model_batch.add(Flatten(input_shape = (32,32,1))) #input_shape defines the shape of the input: 32X32 is the image size, while 3 is the number of dimensions (RGB hence 3)\n",
    "new_a_model_batch.add(leaky_relu)\n",
    "new_a_model_batch.add(BatchNormalization())\n",
    "new_a_model_batch.add(Dense(n_filters_start,kernel_initializer = he_normal_initializer)) \n",
    "new_a_model_batch.add(leaky_relu)\n",
    "new_a_model_batch.add(BatchNormalization())\n",
    "new_a_model_batch.add(Dropout(dropout))\n",
    "new_a_model_batch.add(BatchNormalization())\n",
    "new_a_model_batch.add(Dense(n_filters_start/2))\n",
    "new_a_model_batch.add(leaky_relu)\n",
    "new_a_model_batch.add(BatchNormalization())\n",
    "new_a_model_batch.add(Dropout(dropout))\n",
    "new_a_model_batch.add(BatchNormalization())\n",
    "new_a_model_batch.add(Dense(n_filters_finish))\n",
    "new_a_model_batch.add(BatchNormalization())\n",
    "new_a_model_batch.add(Dense(n_filters_finish,activation='softmax'))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "#Compile the network: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 175 samples\n",
      "Epoch 1/50\n",
      "6474/6474 [==============================] - 4s 646us/sample - loss: 1.2095 - accuracy: 0.4583 - val_loss: 1.2938 - val_accuracy: 0.3657\n",
      "Epoch 2/50\n",
      "6474/6474 [==============================] - 2s 255us/sample - loss: 0.9847 - accuracy: 0.6091 - val_loss: 1.1088 - val_accuracy: 0.5429\n",
      "Epoch 3/50\n",
      "6474/6474 [==============================] - 2s 264us/sample - loss: 0.8827 - accuracy: 0.6823 - val_loss: 1.0235 - val_accuracy: 0.6457\n",
      "Epoch 4/50\n",
      "6474/6474 [==============================] - 2s 263us/sample - loss: 0.8324 - accuracy: 0.7212 - val_loss: 0.9809 - val_accuracy: 0.6571\n",
      "Epoch 5/50\n",
      "6474/6474 [==============================] - 2s 266us/sample - loss: 0.8051 - accuracy: 0.7476 - val_loss: 0.9534 - val_accuracy: 0.6857\n",
      "Epoch 6/50\n",
      "6474/6474 [==============================] - 2s 267us/sample - loss: 0.7808 - accuracy: 0.7680 - val_loss: 0.9356 - val_accuracy: 0.7086\n",
      "Epoch 7/50\n",
      "6474/6474 [==============================] - 2s 266us/sample - loss: 0.7599 - accuracy: 0.7841 - val_loss: 0.9168 - val_accuracy: 0.7143\n",
      "Epoch 8/50\n",
      "6474/6474 [==============================] - 2s 261us/sample - loss: 0.7342 - accuracy: 0.8069 - val_loss: 0.9097 - val_accuracy: 0.7143\n",
      "Epoch 9/50\n",
      "6474/6474 [==============================] - 2s 255us/sample - loss: 0.7246 - accuracy: 0.8085 - val_loss: 0.8906 - val_accuracy: 0.7371\n",
      "Epoch 10/50\n",
      "6474/6474 [==============================] - 2s 255us/sample - loss: 0.7160 - accuracy: 0.8114 - val_loss: 0.8918 - val_accuracy: 0.7371\n",
      "Epoch 11/50\n",
      "6474/6474 [==============================] - 2s 271us/sample - loss: 0.6945 - accuracy: 0.8282 - val_loss: 0.8823 - val_accuracy: 0.7314\n",
      "Epoch 12/50\n",
      "6474/6474 [==============================] - 2s 278us/sample - loss: 0.6853 - accuracy: 0.8250 - val_loss: 0.8809 - val_accuracy: 0.7371\n",
      "Epoch 13/50\n",
      "6474/6474 [==============================] - 2s 260us/sample - loss: 0.6812 - accuracy: 0.8299 - val_loss: 0.8753 - val_accuracy: 0.7257\n",
      "Epoch 14/50\n",
      "6474/6474 [==============================] - 2s 254us/sample - loss: 0.6789 - accuracy: 0.8355 - val_loss: 0.8762 - val_accuracy: 0.7314\n",
      "Epoch 15/50\n",
      "6474/6474 [==============================] - 2s 259us/sample - loss: 0.6614 - accuracy: 0.8389 - val_loss: 0.8663 - val_accuracy: 0.7314\n",
      "Epoch 16/50\n",
      "6474/6474 [==============================] - 2s 272us/sample - loss: 0.6525 - accuracy: 0.8454 - val_loss: 0.8707 - val_accuracy: 0.7371\n",
      "Epoch 17/50\n",
      "6474/6474 [==============================] - 2s 266us/sample - loss: 0.6461 - accuracy: 0.8448 - val_loss: 0.8592 - val_accuracy: 0.7314\n",
      "Epoch 18/50\n",
      "6474/6474 [==============================] - 2s 253us/sample - loss: 0.6428 - accuracy: 0.8482 - val_loss: 0.8583 - val_accuracy: 0.7371\n",
      "Epoch 19/50\n",
      "6474/6474 [==============================] - 2s 250us/sample - loss: 0.6319 - accuracy: 0.8582 - val_loss: 0.8645 - val_accuracy: 0.7257\n",
      "Epoch 20/50\n",
      "6474/6474 [==============================] - 2s 267us/sample - loss: 0.6267 - accuracy: 0.8563 - val_loss: 0.8600 - val_accuracy: 0.7314\n",
      "Epoch 21/50\n",
      "6474/6474 [==============================] - 2s 263us/sample - loss: 0.6245 - accuracy: 0.8565 - val_loss: 0.8632 - val_accuracy: 0.7257\n",
      "Epoch 22/50\n",
      "6474/6474 [==============================] - 2s 254us/sample - loss: 0.6166 - accuracy: 0.8636 - val_loss: 0.8582 - val_accuracy: 0.7257\n",
      "Epoch 23/50\n",
      "6474/6474 [==============================] - 2s 259us/sample - loss: 0.6132 - accuracy: 0.8610 - val_loss: 0.8585 - val_accuracy: 0.7257\n",
      "Epoch 24/50\n",
      "6474/6474 [==============================] - 2s 268us/sample - loss: 0.6081 - accuracy: 0.8548 - val_loss: 0.8544 - val_accuracy: 0.7257\n",
      "Epoch 25/50\n",
      "6474/6474 [==============================] - 2s 248us/sample - loss: 0.6026 - accuracy: 0.8627 - val_loss: 0.8470 - val_accuracy: 0.7257\n",
      "Epoch 26/50\n",
      "6474/6474 [==============================] - 2s 261us/sample - loss: 0.5951 - accuracy: 0.8664 - val_loss: 0.8436 - val_accuracy: 0.7257\n",
      "Epoch 27/50\n",
      "6474/6474 [==============================] - 2s 252us/sample - loss: 0.5874 - accuracy: 0.8703 - val_loss: 0.8393 - val_accuracy: 0.7371\n",
      "Epoch 28/50\n",
      "6474/6474 [==============================] - 2s 266us/sample - loss: 0.5852 - accuracy: 0.8693 - val_loss: 0.8393 - val_accuracy: 0.7314\n",
      "Epoch 29/50\n",
      "6474/6474 [==============================] - 2s 277us/sample - loss: 0.5814 - accuracy: 0.8684 - val_loss: 0.8444 - val_accuracy: 0.7314\n",
      "Epoch 30/50\n",
      "6474/6474 [==============================] - 2s 267us/sample - loss: 0.5805 - accuracy: 0.8676 - val_loss: 0.8406 - val_accuracy: 0.7314\n",
      "Epoch 31/50\n",
      "6474/6474 [==============================] - 2s 254us/sample - loss: 0.5699 - accuracy: 0.8775 - val_loss: 0.8428 - val_accuracy: 0.7200\n",
      "Epoch 32/50\n",
      "6474/6474 [==============================] - 2s 274us/sample - loss: 0.5671 - accuracy: 0.8763 - val_loss: 0.8398 - val_accuracy: 0.7257\n",
      "Epoch 33/50\n",
      "6474/6474 [==============================] - 2s 274us/sample - loss: 0.5667 - accuracy: 0.8699 - val_loss: 0.8383 - val_accuracy: 0.7143\n",
      "Epoch 34/50\n",
      "6474/6474 [==============================] - 2s 272us/sample - loss: 0.5631 - accuracy: 0.8712 - val_loss: 0.8393 - val_accuracy: 0.7257\n",
      "Epoch 35/50\n",
      "6474/6474 [==============================] - 2s 274us/sample - loss: 0.5484 - accuracy: 0.8851 - val_loss: 0.8421 - val_accuracy: 0.7200\n",
      "Epoch 36/50\n",
      "6474/6474 [==============================] - 2s 270us/sample - loss: 0.5466 - accuracy: 0.8795 - val_loss: 0.8370 - val_accuracy: 0.7200\n",
      "Epoch 37/50\n",
      "6474/6474 [==============================] - 2s 280us/sample - loss: 0.5529 - accuracy: 0.8812 - val_loss: 0.8364 - val_accuracy: 0.7314\n",
      "Epoch 38/50\n",
      "6474/6474 [==============================] - 2s 264us/sample - loss: 0.5448 - accuracy: 0.8753 - val_loss: 0.8278 - val_accuracy: 0.7314\n",
      "Epoch 39/50\n",
      "6474/6474 [==============================] - 1s 225us/sample - loss: 0.5390 - accuracy: 0.8835 - val_loss: 0.8270 - val_accuracy: 0.7371\n",
      "Epoch 40/50\n",
      "6474/6474 [==============================] - 2s 233us/sample - loss: 0.5414 - accuracy: 0.8795 - val_loss: 0.8292 - val_accuracy: 0.7257\n",
      "Epoch 41/50\n",
      "6474/6474 [==============================] - 2s 324us/sample - loss: 0.5303 - accuracy: 0.8888 - val_loss: 0.8264 - val_accuracy: 0.7257\n",
      "Epoch 42/50\n",
      "6474/6474 [==============================] - 2s 280us/sample - loss: 0.5260 - accuracy: 0.8872 - val_loss: 0.8312 - val_accuracy: 0.7200\n",
      "Epoch 43/50\n",
      "6474/6474 [==============================] - 2s 266us/sample - loss: 0.5270 - accuracy: 0.8863 - val_loss: 0.8290 - val_accuracy: 0.7200\n",
      "Epoch 44/50\n",
      "6474/6474 [==============================] - 2s 259us/sample - loss: 0.5197 - accuracy: 0.8892 - val_loss: 0.8265 - val_accuracy: 0.7200\n",
      "Epoch 45/50\n",
      "6474/6474 [==============================] - 2s 270us/sample - loss: 0.5128 - accuracy: 0.8923 - val_loss: 0.8251 - val_accuracy: 0.7257\n",
      "Epoch 46/50\n",
      "6474/6474 [==============================] - 2s 273us/sample - loss: 0.5146 - accuracy: 0.8885 - val_loss: 0.8246 - val_accuracy: 0.7200\n",
      "Epoch 47/50\n",
      "6474/6474 [==============================] - 2s 279us/sample - loss: 0.5127 - accuracy: 0.8899 - val_loss: 0.8271 - val_accuracy: 0.7257\n",
      "Epoch 48/50\n",
      "6474/6474 [==============================] - 2s 274us/sample - loss: 0.5047 - accuracy: 0.8934 - val_loss: 0.8254 - val_accuracy: 0.7200\n",
      "Epoch 49/50\n",
      "6474/6474 [==============================] - 2s 274us/sample - loss: 0.5051 - accuracy: 0.8950 - val_loss: 0.8220 - val_accuracy: 0.7257\n",
      "Epoch 50/50\n",
      "6474/6474 [==============================] - 2s 264us/sample - loss: 0.4988 - accuracy: 0.8925 - val_loss: 0.8205 - val_accuracy: 0.7257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b8a34f6908>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model_batch.compile(optimizer = AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "new_a_model_batch.fit(BaseX_train, BaseY_train, epochs = epochs, batch_size = batch_size, validation_data=(X_test,Y_test))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at results/model_relu_32.h5 \n",
      "Saved trained model at results/new_a_model_batch_norm.h5 \n"
     ]
    }
   ],
   "source": [
    "#model_relu - batch = 32:\n",
    "model_name = \"model_relu_32.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "#new_a_model - with batch normalization:\n",
    "model_name = \"new_a_model_batch_norm.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model_batch.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Convolutional Neural Network (CNN)\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *2D CNN.* \n",
    "\n",
    "Have a look at the model below and answer the following:\n",
    "* How many layers does it have?\n",
    "* How many filter in each layer?\n",
    "* Would the number of parmaters be similar to a fully connected NN?\n",
    "* Is this specific NN performing regularization?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(input_shape,drop,dropRate,reg):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "NNet = get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNet=get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "#Saving checkpoints during training:\n",
    "Checkpath = os.getcwd()\n",
    "Checkp = ModelCheckpoint(Checkpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 121s 19ms/sample - loss: 8.1043 - acc: 0.4272 - val_loss: 7.8786 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 7.6432 - acc: 0.5310 - val_loss: 7.9201 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 7.4605 - acc: 0.5884 - val_loss: 8.0081 - val_acc: 0.2517\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 119s 18ms/sample - loss: 7.3379 - acc: 0.6118 - val_loss: 8.0063 - val_acc: 0.2575\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 120s 18ms/sample - loss: 7.2537 - acc: 0.6359 - val_loss: 7.8991 - val_acc: 0.2708\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 119s 18ms/sample - loss: 7.1866 - acc: 0.6555 - val_loss: 7.7292 - val_acc: 0.3229\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 120s 19ms/sample - loss: 7.1363 - acc: 0.6670 - val_loss: 7.6668 - val_acc: 0.3611\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 120s 19ms/sample - loss: 7.0802 - acc: 0.6867 - val_loss: 7.5847 - val_acc: 0.4091\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 119s 18ms/sample - loss: 7.0392 - acc: 0.6959 - val_loss: 7.6303 - val_acc: 0.3900\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.9682 - acc: 0.7179 - val_loss: 7.5902 - val_acc: 0.3924\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.9316 - acc: 0.7257 - val_loss: 7.5740 - val_acc: 0.3929\n",
      "Epoch 12/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.8982 - acc: 0.7266 - val_loss: 7.5689 - val_acc: 0.3866\n",
      "Epoch 13/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.8628 - acc: 0.7391 - val_loss: 7.5688 - val_acc: 0.3796\n",
      "Epoch 14/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.8231 - acc: 0.7470 - val_loss: 7.5605 - val_acc: 0.3756\n",
      "Epoch 15/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.7919 - acc: 0.7553 - val_loss: 7.5805 - val_acc: 0.3767\n",
      "Epoch 16/25\n",
      "6474/6474 [==============================] - 119s 18ms/sample - loss: 6.7756 - acc: 0.7556 - val_loss: 7.5463 - val_acc: 0.3819\n",
      "Epoch 17/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.7200 - acc: 0.7688 - val_loss: 7.5981 - val_acc: 0.3750\n",
      "Epoch 18/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.6953 - acc: 0.7762 - val_loss: 7.5548 - val_acc: 0.3773\n",
      "Epoch 19/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.6678 - acc: 0.7799 - val_loss: 7.5341 - val_acc: 0.3802\n",
      "Epoch 20/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.6577 - acc: 0.7822 - val_loss: 7.5193 - val_acc: 0.3848\n",
      "Epoch 21/25\n",
      "6474/6474 [==============================] - 119s 18ms/sample - loss: 6.6379 - acc: 0.7824 - val_loss: 7.4892 - val_acc: 0.3883\n",
      "Epoch 22/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.6035 - acc: 0.7888 - val_loss: 7.5243 - val_acc: 0.3814\n",
      "Epoch 23/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.5743 - acc: 0.7902 - val_loss: 7.4871 - val_acc: 0.3912\n",
      "Epoch 24/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.5690 - acc: 0.7902 - val_loss: 7.5152 - val_acc: 0.3843\n",
      "Epoch 25/25\n",
      "6474/6474 [==============================] - 118s 18ms/sample - loss: 6.5254 - acc: 0.8060 - val_loss: 7.4866 - val_acc: 0.3854\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "# IMPORTANT NOTE: This will take a few minutes!\n",
    "h = NNet.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "#NNet.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# NNet.load_weights('Weights_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at results/NNet.h5 \n"
     ]
    }
   ],
   "source": [
    "#new_a_model - with batch normalization:\n",
    "model_name = \"NNet.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "NNet.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 1s 8ms/sample - loss: 7.9831 - acc: 0.2971\n",
      "test loss, test acc: [7.983084681374686, 0.29714286]\n"
     ]
    }
   ],
   "source": [
    "results = NNet.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Number of filters* \n",
    "\n",
    "Rebuild the function `get_net` to have as an input argument a list of number of filters in each layers, i.e. for the CNN defined above the input should have been `[64, 128, 128, 256, 256]`. Now train the model with the number of filters reduced by half. What were the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "def get_net(input_shape,drop,dropRate,reg, filter_num1, filter_num2, filter_num3, filter_num4, filter_num5):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=filter_num1, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=filter_num2, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=filter_num3, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=filter_num4, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=filter_num5, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,392,772\n",
      "Trainable params: 1,392,612\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_n1 = 32\n",
    "filter_n2 = 64\n",
    "filter_n3 = 64\n",
    "filter_n4 = 128\n",
    "filter_n5 = 128\n",
    "\n",
    "NNet_half = get_net(input_shape,drop,dropRate,reg, filter_n1, filter_n2, filter_n3, filter_n4, filter_n5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 55s 9ms/sample - loss: 5.0151 - acc: 0.3593 - val_loss: 4.6334 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 53s 8ms/sample - loss: 4.5919 - acc: 0.4804 - val_loss: 4.7241 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 47s 7ms/sample - loss: 4.4434 - acc: 0.5256 - val_loss: 4.7973 - val_acc: 0.2500\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 49s 8ms/sample - loss: 4.3581 - acc: 0.5462 - val_loss: 4.7466 - val_acc: 0.2668\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 46s 7ms/sample - loss: 4.2522 - acc: 0.5783 - val_loss: 4.6592 - val_acc: 0.2963\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 46s 7ms/sample - loss: 4.2161 - acc: 0.5853 - val_loss: 4.5386 - val_acc: 0.3461\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 46s 7ms/sample - loss: 4.1602 - acc: 0.6109 - val_loss: 4.4724 - val_acc: 0.4005\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 50s 8ms/sample - loss: 4.1120 - acc: 0.6277 - val_loss: 4.4419 - val_acc: 0.4271\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 54s 8ms/sample - loss: 4.0803 - acc: 0.6293 - val_loss: 4.4247 - val_acc: 0.4375\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 49s 8ms/sample - loss: 4.0444 - acc: 0.6484 - val_loss: 4.4258 - val_acc: 0.4311\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 57s 9ms/sample - loss: 4.0306 - acc: 0.6443 - val_loss: 4.4197 - val_acc: 0.4253\n",
      "Epoch 12/25\n",
      "6474/6474 [==============================] - 49s 8ms/sample - loss: 4.0127 - acc: 0.6574 - val_loss: 4.4276 - val_acc: 0.4126\n",
      "Epoch 13/25\n",
      "6474/6474 [==============================] - 47s 7ms/sample - loss: 3.9885 - acc: 0.6702 - val_loss: 4.4305 - val_acc: 0.4155\n",
      "Epoch 14/25\n",
      "6474/6474 [==============================] - 49s 8ms/sample - loss: 3.9510 - acc: 0.6762 - val_loss: 4.4282 - val_acc: 0.4097\n",
      "Epoch 15/25\n",
      "6474/6474 [==============================] - 49s 8ms/sample - loss: 3.9272 - acc: 0.6820 - val_loss: 4.4277 - val_acc: 0.4005\n",
      "Epoch 16/25\n",
      "6474/6474 [==============================] - 47s 7ms/sample - loss: 3.9167 - acc: 0.6869 - val_loss: 4.4296 - val_acc: 0.3935\n",
      "Epoch 17/25\n",
      "6474/6474 [==============================] - 47s 7ms/sample - loss: 3.9038 - acc: 0.6931 - val_loss: 4.4253 - val_acc: 0.4051\n",
      "Epoch 18/25\n",
      "6474/6474 [==============================] - 50s 8ms/sample - loss: 3.8851 - acc: 0.6917 - val_loss: 4.4257 - val_acc: 0.3964\n",
      "Epoch 19/25\n",
      "6474/6474 [==============================] - 48s 7ms/sample - loss: 3.8693 - acc: 0.6988 - val_loss: 4.4205 - val_acc: 0.4045\n",
      "Epoch 20/25\n",
      "6474/6474 [==============================] - 60s 9ms/sample - loss: 3.8329 - acc: 0.7070 - val_loss: 4.4233 - val_acc: 0.3970\n",
      "Epoch 21/25\n",
      "6474/6474 [==============================] - 67s 10ms/sample - loss: 3.8308 - acc: 0.7122 - val_loss: 4.4124 - val_acc: 0.4109\n",
      "Epoch 22/25\n",
      "6474/6474 [==============================] - 51s 8ms/sample - loss: 3.8302 - acc: 0.7138 - val_loss: 4.4124 - val_acc: 0.4028\n",
      "Epoch 23/25\n",
      "6474/6474 [==============================] - 47s 7ms/sample - loss: 3.8071 - acc: 0.7158 - val_loss: 4.4096 - val_acc: 0.4115\n",
      "Epoch 24/25\n",
      "6474/6474 [==============================] - 48s 7ms/sample - loss: 3.8082 - acc: 0.7076 - val_loss: 4.4036 - val_acc: 0.4115\n",
      "Epoch 25/25\n",
      "6474/6474 [==============================] - 48s 7ms/sample - loss: 3.7794 - acc: 0.7281 - val_loss: 4.4050 - val_acc: 0.4103\n"
     ]
    }
   ],
   "source": [
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet_half.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "#Saving checkpoints during training:\n",
    "Checkpath = os.getcwd()\n",
    "Checkp = ModelCheckpoint(Checkpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, save_freq=1)\n",
    "\n",
    "#Preforming the training by using fit \n",
    "# IMPORTANT NOTE: This will take a few minutes!\n",
    "h = NNet_half.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "#NNet_half.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at results/NNet_half.h5 \n"
     ]
    }
   ],
   "source": [
    "#new_a_model - with batch normalization:\n",
    "model_name = \"NNet_half.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "NNet_half.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 1s 3ms/sample - loss: 4.6617 - acc: 0.3714\n",
      "test loss, test acc: [4.661678107125419, 0.37142858]\n"
     ]
    }
   ],
   "source": [
    "results = NNet_half.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! See you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
